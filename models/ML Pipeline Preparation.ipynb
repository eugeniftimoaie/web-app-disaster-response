{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\eugen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eugen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords'])\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/drp.db')\n",
    "df = pd.read_sql_table('FigureEight_data', con = engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "Y = df.iloc[:, 4:]\n",
    "category_names = list(df.columns[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- X -----------------------------------------\n",
      "We ARE in Port-Margot, in the Northern part of the Country. It is 2hrs driving from Cap-Haitian, We do not have anything to survive. We are dying, please help us.\n",
      "-------- Target variables Y ------------------------\n",
      "related                   1\n",
      "request                   1\n",
      "offer                     0\n",
      "aid_related               1\n",
      "medical_help              1\n",
      "medical_products          0\n",
      "search_and_rescue         0\n",
      "security                  0\n",
      "military                  0\n",
      "child_alone               0\n",
      "water                     0\n",
      "food                      1\n",
      "shelter                   0\n",
      "clothing                  0\n",
      "money                     0\n",
      "missing_people            0\n",
      "refugees                  0\n",
      "death                     1\n",
      "other_aid                 1\n",
      "infrastructure_related    0\n",
      "transport                 0\n",
      "buildings                 0\n",
      "electricity               0\n",
      "tools                     0\n",
      "hospitals                 0\n",
      "shops                     0\n",
      "aid_centers               0\n",
      "other_infrastructure      0\n",
      "weather_related           0\n",
      "floods                    0\n",
      "storm                     0\n",
      "fire                      0\n",
      "earthquake                0\n",
      "cold                      0\n",
      "other_weather             0\n",
      "direct_report             1\n",
      "Name: 200, dtype: int64\n",
      "-------- Category names ----------------------------\n",
      "['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n"
     ]
    }
   ],
   "source": [
    "i = 200\n",
    "print('X:')\n",
    "print(X.iloc[i])\n",
    "print('Target variables Y:')\n",
    "print(Y.iloc[i])\n",
    "print('Category names:')\n",
    "print(category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text and innitiate lemmatizer\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "    \n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        # lemmatize and remove leading/ trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We ARE in Port-Margot, in the Northern part of the Country. It is 2hrs driving from Cap-Haitian, We do not have anything to survive. We are dying, please help us.\n",
      "['port', 'margot', 'northern', 'part', 'country', '2hrs', 'driving', 'cap', 'haitian', 'anything', 'survive', 'dying', 'please', 'help', 'u']\n"
     ]
    }
   ],
   "source": [
    "i = 200\n",
    "print(X.iloc[i])\n",
    "print(tokenize(X.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_01 = Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    #('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: X_train (20972,) | y_train (20972, 36)\n",
      "Shape of testing set: X_train (5244,) | y_train (5244, 36)\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "print('Shape of training set: X_train {} | y_train {}'.format(X_train.shape, y_train.shape))\n",
    "print('Shape of testing set: X_train {} | y_train {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                 MultiOutputClassifier(estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features=None,\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        presort='deprecated',\n",
       "                                                                        random_state=None,\n",
       "                                                                        splitter='best'),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "pipeline_01.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "y_pred_01 = pipeline_01.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to report f1 score, precision and recall for each output category\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    results = pd.DataFrame(columns = ['Category', 'f1_score', 'precision', 'recall'])\n",
    "    \n",
    "    i = 0\n",
    "    for category in y_test.columns:\n",
    "        precision, recall, f1_score, support = precision_recall_fscore_support(y_test[category], \n",
    "                                                                               y_pred[:,i], \n",
    "                                                                               average = 'weighted')\n",
    "        results.at[i + 1, 'Category'] = category\n",
    "        results.at[i + 1, 'f1_score'] = f1_score\n",
    "        results.at[i + 1, 'precision'] = precision\n",
    "        results.at[i + 1, 'recall'] = recall\n",
    "        i += 1\n",
    "    print('Overall f1_score: ', '{:.4}'.format(results['f1_score'].mean()))\n",
    "    print('Overall precision: ', '{:.4}'.format(results['precision'].mean()))\n",
    "    print('Overall recall: ', '{:.4}'.format(results['recall'].mean()))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall f1_score:  0.9351\n",
      "Overall precision:  0.9332\n",
      "Overall recall:  0.9376\n"
     ]
    }
   ],
   "source": [
    "results_p01 = evaluate_model(y_test, y_pred_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>related</td>\n",
       "      <td>0.762833</td>\n",
       "      <td>0.763941</td>\n",
       "      <td>0.763539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>request</td>\n",
       "      <td>0.860944</td>\n",
       "      <td>0.858161</td>\n",
       "      <td>0.86537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer</td>\n",
       "      <td>0.991806</td>\n",
       "      <td>0.990101</td>\n",
       "      <td>0.993516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aid_related</td>\n",
       "      <td>0.721287</td>\n",
       "      <td>0.720809</td>\n",
       "      <td>0.721968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical_help</td>\n",
       "      <td>0.905505</td>\n",
       "      <td>0.901116</td>\n",
       "      <td>0.911327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical_products</td>\n",
       "      <td>0.946573</td>\n",
       "      <td>0.944015</td>\n",
       "      <td>0.95061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>search_and_rescue</td>\n",
       "      <td>0.965249</td>\n",
       "      <td>0.963643</td>\n",
       "      <td>0.96701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>security</td>\n",
       "      <td>0.969892</td>\n",
       "      <td>0.967856</td>\n",
       "      <td>0.971968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>military</td>\n",
       "      <td>0.964531</td>\n",
       "      <td>0.964531</td>\n",
       "      <td>0.964531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>child_alone</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>water</td>\n",
       "      <td>0.960551</td>\n",
       "      <td>0.960233</td>\n",
       "      <td>0.960908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>food</td>\n",
       "      <td>0.943602</td>\n",
       "      <td>0.943168</td>\n",
       "      <td>0.944127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shelter</td>\n",
       "      <td>0.928727</td>\n",
       "      <td>0.927514</td>\n",
       "      <td>0.930206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clothing</td>\n",
       "      <td>0.98771</td>\n",
       "      <td>0.987254</td>\n",
       "      <td>0.988368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>money</td>\n",
       "      <td>0.975042</td>\n",
       "      <td>0.975463</td>\n",
       "      <td>0.974638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>missing_people</td>\n",
       "      <td>0.985723</td>\n",
       "      <td>0.984654</td>\n",
       "      <td>0.987223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>refugees</td>\n",
       "      <td>0.960955</td>\n",
       "      <td>0.960115</td>\n",
       "      <td>0.961861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>death</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.966336</td>\n",
       "      <td>0.967582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other_aid</td>\n",
       "      <td>0.821344</td>\n",
       "      <td>0.81284</td>\n",
       "      <td>0.831998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>infrastructure_related</td>\n",
       "      <td>0.905345</td>\n",
       "      <td>0.896809</td>\n",
       "      <td>0.915523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>transport</td>\n",
       "      <td>0.94017</td>\n",
       "      <td>0.936535</td>\n",
       "      <td>0.944699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>buildings</td>\n",
       "      <td>0.950123</td>\n",
       "      <td>0.947991</td>\n",
       "      <td>0.953471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>electricity</td>\n",
       "      <td>0.972821</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>0.9754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tools</td>\n",
       "      <td>0.989587</td>\n",
       "      <td>0.988743</td>\n",
       "      <td>0.990465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.984423</td>\n",
       "      <td>0.983733</td>\n",
       "      <td>0.985126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shops</td>\n",
       "      <td>0.992566</td>\n",
       "      <td>0.991617</td>\n",
       "      <td>0.993516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aid_centers</td>\n",
       "      <td>0.980612</td>\n",
       "      <td>0.978407</td>\n",
       "      <td>0.983219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>other_infrastructure</td>\n",
       "      <td>0.934485</td>\n",
       "      <td>0.92757</td>\n",
       "      <td>0.942601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>weather_related</td>\n",
       "      <td>0.85828</td>\n",
       "      <td>0.857402</td>\n",
       "      <td>0.860221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>floods</td>\n",
       "      <td>0.943069</td>\n",
       "      <td>0.941812</td>\n",
       "      <td>0.944889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>storm</td>\n",
       "      <td>0.936954</td>\n",
       "      <td>0.93684</td>\n",
       "      <td>0.937071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fire</td>\n",
       "      <td>0.985807</td>\n",
       "      <td>0.985051</td>\n",
       "      <td>0.986651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.957862</td>\n",
       "      <td>0.957438</td>\n",
       "      <td>0.958429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cold</td>\n",
       "      <td>0.979611</td>\n",
       "      <td>0.978829</td>\n",
       "      <td>0.98074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>other_weather</td>\n",
       "      <td>0.93083</td>\n",
       "      <td>0.927211</td>\n",
       "      <td>0.935164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>direct_report</td>\n",
       "      <td>0.802232</td>\n",
       "      <td>0.797114</td>\n",
       "      <td>0.810069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Category  f1_score precision    recall\n",
       "1                  related  0.762833  0.763941  0.763539\n",
       "2                  request  0.860944  0.858161   0.86537\n",
       "3                    offer  0.991806  0.990101  0.993516\n",
       "4              aid_related  0.721287  0.720809  0.721968\n",
       "5             medical_help  0.905505  0.901116  0.911327\n",
       "6         medical_products  0.946573  0.944015   0.95061\n",
       "7        search_and_rescue  0.965249  0.963643   0.96701\n",
       "8                 security  0.969892  0.967856  0.971968\n",
       "9                 military  0.964531  0.964531  0.964531\n",
       "10             child_alone         1         1         1\n",
       "11                   water  0.960551  0.960233  0.960908\n",
       "12                    food  0.943602  0.943168  0.944127\n",
       "13                 shelter  0.928727  0.927514  0.930206\n",
       "14                clothing   0.98771  0.987254  0.988368\n",
       "15                   money  0.975042  0.975463  0.974638\n",
       "16          missing_people  0.985723  0.984654  0.987223\n",
       "17                refugees  0.960955  0.960115  0.961861\n",
       "18                   death    0.9669  0.966336  0.967582\n",
       "19               other_aid  0.821344   0.81284  0.831998\n",
       "20  infrastructure_related  0.905345  0.896809  0.915523\n",
       "21               transport   0.94017  0.936535  0.944699\n",
       "22               buildings  0.950123  0.947991  0.953471\n",
       "23             electricity  0.972821  0.971074    0.9754\n",
       "24                   tools  0.989587  0.988743  0.990465\n",
       "25               hospitals  0.984423  0.983733  0.985126\n",
       "26                   shops  0.992566  0.991617  0.993516\n",
       "27             aid_centers  0.980612  0.978407  0.983219\n",
       "28    other_infrastructure  0.934485   0.92757  0.942601\n",
       "29         weather_related   0.85828  0.857402  0.860221\n",
       "30                  floods  0.943069  0.941812  0.944889\n",
       "31                   storm  0.936954   0.93684  0.937071\n",
       "32                    fire  0.985807  0.985051  0.986651\n",
       "33              earthquake  0.957862  0.957438  0.958429\n",
       "34                    cold  0.979611  0.978829   0.98074\n",
       "35           other_weather   0.93083  0.927211  0.935164\n",
       "36           direct_report  0.802232  0.797114  0.810069"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_p01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x000002BF98E678B8>,\n",
       "                   vocabulary=None)),\n",
       "  ('clf', MultiOutputClassifier(estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features=None,\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          presort='deprecated',\n",
       "                                                          random_state=None,\n",
       "                                                          splitter='best'),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x000002BF98E678B8>,\n",
       "                 vocabulary=None),\n",
       " 'clf': MultiOutputClassifier(estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort='deprecated',\n",
       "                                                        random_state=None,\n",
       "                                                        splitter='best'),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': None,\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__presort': 'deprecated',\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__splitter': 'best',\n",
       " 'clf__estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_01.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "            'clf__estimator__criterion': ['gini', 'entropy']   \n",
    "            }\n",
    "\n",
    "cv_01 = GridSearchCV(estimator = pipeline_01, param_grid = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(...\n",
       "                                                                                               min_impurity_split=None,\n",
       "                                                                                               min_samples_leaf=1,\n",
       "                                                                                               min_samples_split=2,\n",
       "                                                                                               min_weight_fraction_leaf=0.0,\n",
       "                                                                                               presort='deprecated',\n",
       "                                                                                               random_state=None,\n",
       "                                                                                               splitter='best'),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'clf__estimator__criterion': ['gini', 'entropy']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "cv_01.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv_01 = cv_01.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall f1_score:  0.9349\n",
      "Overall precision:  0.933\n",
      "Overall recall:  0.9374\n"
     ]
    }
   ],
   "source": [
    "results_cv01 = evaluate_model(y_test, y_pred_cv_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model improvement with GridSearchCV has brought no amelioration of the results. Results of f1_score is even slightly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline_02: DecisionTreeClassifier + TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipeline_02 = Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                 MultiOutputClassifier(estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features=None,\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        presort='deprecated',\n",
       "                                                                        random_state=None,\n",
       "                                                                        splitter='best'),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "pipeline_02.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "y_pred_02 = pipeline_02.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall f1_score:  0.9317\n",
      "Overall precision:  0.9308\n",
      "Overall recall:  0.9328\n"
     ]
    }
   ],
   "source": [
    "results_p02 = evaluate_model(y_test, y_pred_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline_03: RandomForestClassifier + TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipeline_03 = Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "pipeline_03.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "y_pred_03 = pipeline_03.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall f1_score:  0.9366\n",
      "Overall precision:  0.9387\n",
      "Overall recall:  0.9483\n"
     ]
    }
   ],
   "source": [
    "results_p03 = evaluate_model(y_test, y_pred_03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline_04: RandomForestClassifier without TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipeline_04 = Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    #('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "pipeline_04.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "y_pred_04 = pipeline_04.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall f1_score:  0.9379\n",
      "Overall precision:  0.9402\n",
      "Overall recall:  0.9486\n"
     ]
    }
   ],
   "source": [
    "results_p04 = evaluate_model(y_test, y_pred_04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_01.pkl']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline_01, \"classifier_01.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_01_cv.pkl']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv_01, \"classifier_01_cv.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_02.pkl']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline_02, \"classifier_02.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_03.pkl']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline_03, \"classifier_03.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_04.pkl']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline_04, \"classifier_04.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
